{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3312aea6-1674-4ad7-8bd9-c13e16c2e59f",
   "metadata": {
    "id": "3312aea6-1674-4ad7-8bd9-c13e16c2e59f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hydro\\anaconda3\\envs\\cov\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "import random\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7a210e-d08f-4eee-bdb9-fadef5bf6a39",
   "metadata": {
    "id": "af7a210e-d08f-4eee-bdb9-fadef5bf6a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step2-output-FR-2020-06-01.csv', 'step2-output-FR-2020-06-02.csv', 'step2-output-FR-2020-06-04.csv', 'step2-output-FR-2020-06-05.csv', 'step2-output-FR-2020-06-06.csv', 'step2-output-FR-2020-06-07.csv', 'step2-output-FR-2020-06-08.csv', 'step2-output-FR-2020-06-09.csv', 'step2-output-FR-2020-06-10.csv', 'step2-output-FR-2020-06-11.csv', 'step2-output-FR-2020-06-12.csv', 'step2-output-FR-2020-06-13.csv', 'step2-output-FR-2020-06-14.csv', 'step2-output-FR-2020-06-15.csv', 'step2-output-FR-2020-06-16.csv', 'step2-output-FR-2020-06-17.csv', 'step2-output-FR-2020-06-18.csv', 'step2-output-FR-2020-06-19.csv', 'step2-output-FR-2020-06-21.csv', 'step2-output-FR-2020-06-22.csv', 'step2-output-FR-2020-06-23.csv', 'step2-output-FR-2020-06-24.csv', 'step2-output-FR-2020-06-27.csv', 'step2-output-FR-2020-06-28.csv', 'step2-output-FR-2020-06-29.csv', 'step2-output-FR-2020-06-30.csv']\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-01.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-02.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-04.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-05.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-06.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-07.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-08.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-09.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-10.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-11.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-12.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-13.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-14.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-15.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-16.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-17.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-18.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-19.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-21.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-22.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-23.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-24.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-27.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-28.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-29.csv\n",
      "D:\\covid research\\output\\step 2 output\\2020-06-fr\\step2-output-FR-2020-06-30.csv\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"D:\\covid research\\output\\step 2 output\\2020-06-fr\"\n",
    "days=os.listdir(data_path)\n",
    "print(days)\n",
    "# days =[ i for i in days if i.split('.')[0].split('-')[-1]=='fr']\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for i in days:\n",
    "    file_path  = i\n",
    "    path = os.path.join(data_path, file_path)\n",
    "    print(path)\n",
    "    df_day = pd.read_csv(path,lineterminator='\\n')\n",
    "    data_list.append(df_day)\n",
    "    # print(data_list)\n",
    "df = pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bc33f7-7296-4356-b382-093a1d5c5010",
   "metadata": {
    "id": "58bc33f7-7296-4356-b382-093a1d5c5010",
    "outputId": "71f922cc-7c13-445a-9ffe-58e116a135e6"
   },
   "outputs": [],
   "source": [
    "df_date = df.copy(deep=True)\n",
    "df_date['day'] = df_date['created_at'].apply(lambda x: x.split(' ')[0])\n",
    "df_date['hour'] = df_date['created_at'].apply(lambda x: x.split(' ')[1])\n",
    "df_date['hour'] = df_date['hour'].apply(lambda x: x.split('-')[0])\n",
    "#start_date = '2020-03-' + '%.2d' % start\n",
    "#end_date = '2020-03-' + '%.2d' % (end + 1)\n",
    "\n",
    "#selected_date = (df_date['day'] != start_date) & (df_date['day'] != end_date)\n",
    "df_01 = df_date.copy(deep=True)\n",
    "df_01 = df_01.reset_index(drop=True)\n",
    "day_count = df_01.groupby('day').count()['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb46fad-7e90-408b-9ea6-8e287e3f1ef1",
   "metadata": {
    "id": "8fb46fad-7e90-408b-9ea6-8e287e3f1ef1",
    "outputId": "6b78cd03-b8d8-4026-e12b-e05082b48ec2"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df313d-e863-4f52-8089-66a9c2cdbd3a",
   "metadata": {
    "id": "80df313d-e863-4f52-8089-66a9c2cdbd3a"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65302f0f-d255-44b0-9062-fa93df07e88e",
   "metadata": {
    "id": "65302f0f-d255-44b0-9062-fa93df07e88e"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a741553c-96ab-4734-929a-17e43ebd1e2b",
   "metadata": {
    "id": "a741553c-96ab-4734-929a-17e43ebd1e2b",
    "outputId": "80a7e665-3f48-400b-dc79-6a4c352ed7a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hydro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     rt ryosaeba971 coronavirus cest devenir vasy a...\n",
       "1     _ emmou _ _ nan nan haha cela devoir être imag...\n",
       "2     rt acl_bati coronavirus voir quil plus centre ...\n",
       "3     rt fs0c131y non stopcovid nest installer autom...\n",
       "4     rt oliviertesquet barreau pari déconseille dut...\n",
       "5     rt letriplesept guerre mondial pandémie régula...\n",
       "6     rt ryosaeba971 coronavirus cest devenir vasy a...\n",
       "7     rt chakofr _ _ coronavirus voir quil nest plus...\n",
       "8     covid19 avis français résident passage guatema...\n",
       "9     rt jmd_congo rendez-vous covid19 kinshaser cas...\n",
       "10    rt letriplesept guerre mondial pandémie régula...\n",
       "11    coronavirus ryanair vouloir réduire salaire em...\n",
       "12    rt ryosaeba971 coronavirus cest devenir vasy a...\n",
       "13        jlmelenchon dire covid19 coronagate obamagate\n",
       "14    rt didiermulowa pourquoi unc cache monde entie...\n",
       "15    rt ftmln _ _ _ gar jai besoin avoir cause covi...\n",
       "16    rt usembkinshaser lambassade avoir enregistrer...\n",
       "17    darrenaticsbj cest normal tournage avoir inter...\n",
       "18    cest peu tard aborder sujet coronaviruset si p...\n",
       "19    rt ryosaeba971 coronavirus cest devenir vasy a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01['text'] = df_01['text'].apply(lambda x: remove_emoji(x))\n",
    "df_01['text'] = df_01['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df_01['text'] = df_01['text'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "df_01['text'] = df_01['text'].apply(lambda x: re.sub('\\[[^]]*\\]', '', x))\n",
    "df_01['text'] = df_01['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df_01['text'] = df_01['text'].str.replace('[^\\w\\s]','')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('french'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "df_01['text'] = df_01['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "def remove_xml(text):\n",
    "    return re.sub(r'<[^<]+?>', '', text)\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return text.replace('\\n', ' ') \n",
    "    \n",
    "\n",
    "def remove_manyspaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_xml(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_manyspaces(text)\n",
    "    return text\n",
    "df_01['text']= df_01['text'].apply(clean_text)\n",
    "df_02 = df_01.copy()\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# lemmatizatioin using Spacy to count the appearance of each word\n",
    "def space (comment):\n",
    "    doc = nlp(comment)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "df_02['text']= df_02['text'].apply(space)\n",
    "df_02['text'].head(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03cba9-c067-442d-b925-90f19c2c57c5",
   "metadata": {
    "id": "9c03cba9-c067-442d-b925-90f19c2c57c5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847010fa-8157-4610-894b-2a634205c8dd",
   "metadata": {
    "id": "847010fa-8157-4610-894b-2a634205c8dd"
   },
   "source": [
    "# NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5018861-4678-4f25-8e6d-a6bff9340dd2",
   "metadata": {
    "id": "d5018861-4678-4f25-8e6d-a6bff9340dd2"
   },
   "outputs": [],
   "source": [
    "ncr = pd.read_excel(\"D:/covid research/src/local_exe/step 3/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx\")\n",
    "ncr = ncr[['English (en)', 'French (fr)', 'Positive', 'Negative', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']]\n",
    "ncr = ncr[ncr['French (fr)'] != 'NO TRANSLATION'].reset_index(drop=True)\n",
    "emotions = ['Anger', 'Anticipation','Disgust','Fear', 'Joy','Sadness', 'Surprise', 'Trust']\n",
    "def join_edited_string(edited_tweets):\n",
    "    \n",
    "    edited_string = ''\n",
    "    for row in edited_tweets:\n",
    "        edited_string = edited_string + ' ' + row\n",
    "        \n",
    "    return edited_string\n",
    "\n",
    "joined_string = join_edited_string(df_01['text'])\n",
    "\n",
    "tokens = joined_string.split(' ')\n",
    "\n",
    "joined_string1 = join_edited_string(df_02['text'])\n",
    "tokens1 = joined_string1.split(' ')\n",
    "\n",
    "unique_words = set(tokens)\n",
    "unique_words1 = set(tokens1)\n",
    "unique_words.update(unique_words1)\n",
    "word_to_ind = dict((word, i) for i, word in enumerate(unique_words))\n",
    "ind_to_word = dict((i, word) for i, word in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ecefe1-b8ad-4a7b-ac64-f77861ac1746",
   "metadata": {
    "id": "35ecefe1-b8ad-4a7b-ac64-f77861ac1746",
    "outputId": "f80864e9-8a2f-4b76-a828-9d325eee6c46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hydro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314e874b-82ac-4771-99b0-d81ecf76ba79",
   "metadata": {
    "id": "314e874b-82ac-4771-99b0-d81ecf76ba79"
   },
   "outputs": [],
   "source": [
    "df_tweets = df_01.copy(deep=True)\n",
    "df_tweets1 = df_02.copy(deep=True)\n",
    "emo_info = {'emotion':'' , 'emo_frq': defaultdict(int) }\n",
    "list_emotion_counts = []\n",
    "for emotion in emotions:\n",
    "        emo_info = {}\n",
    "        emo_info['emotion'] = emotion\n",
    "        emo_info['emo_frq'] = defaultdict(int)\n",
    "        list_emotion_counts.append(emo_info)\n",
    "nrow = df_tweets.shape[0]\n",
    "df_emotions = pd.DataFrame(0, index=df_tweets.index, columns=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832aa291-c76a-4396-af02-d1ab550bb283",
   "metadata": {
    "id": "832aa291-c76a-4396-af02-d1ab550bb283"
   },
   "outputs": [],
   "source": [
    "for i in range(nrow):\n",
    "        tweet = word_tokenize(df_tweets.loc[i]['text'], language='french')\n",
    "        for word in tweet:\n",
    "            word_stemmed = word\n",
    "            result = ncr[ncr['French (fr)'] == word_stemmed]\n",
    "            if not result.empty:\n",
    "                # update the tweet-emotions counts\n",
    "                for idx, emotion in enumerate(emotions):\n",
    "                    df_emotions.at[i, emotion] += result[emotion].iloc[0]\n",
    "                    # update the frequencies dictionary list\n",
    "                    if result[emotion].iloc[0].any():\n",
    "                        try:\n",
    "                            list_emotion_counts[idx]['emo_frq'][word_to_ind[word]] += 1\n",
    "                        except:\n",
    "                            continue\n",
    "            else:\n",
    "                doc = nlp(word)\n",
    "                word_stemmed1 = \" \".join([token.lemma_ for token in doc]).lower()\n",
    "                result1 = ncr[ncr['French (fr)'] == word_stemmed1]\n",
    "                if not result1.empty:\n",
    "                    # update the tweet-emotions counts\n",
    "                    for idx, emotion in enumerate(emotions):\n",
    "                        df_emotions.at[i, emotion] += result1[emotion].iloc[0]\n",
    "                        # update the frequencies dictionary list\n",
    "                        if result1[emotion].iloc[0].any():\n",
    "                            try:\n",
    "                                list_emotion_counts[idx]['emo_frq'][word_to_ind[word_stemmed1]] += 1\n",
    "                            except:\n",
    "                                continue\n",
    "            '''\n",
    "            else:\n",
    "                origin = normalize(ft.get_word_vector(word))\n",
    "                product = np.dot(targetvector,origin)\n",
    "                for j in range(len(ncr['French (fr)'])):\n",
    "                        if product[j] > 0.97:\n",
    "                            print(1)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcfb5722-85da-4c1d-bbcf-f5e0a7fa8412",
   "metadata": {
    "id": "fcfb5722-85da-4c1d-bbcf-f5e0a7fa8412"
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.concat([df_tweets, df_emotions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9wMv2Tbh_cQ9",
   "metadata": {
    "id": "9wMv2Tbh_cQ9"
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv(r\"D:\\covid research\\output\\step 3 output\\2020-06-FR.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "French analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cov')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f36b79d7a8a7815ded44fea8921e930581701a037056ac38129bf2cc427d02b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
