{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3312aea6-1674-4ad7-8bd9-c13e16c2e59f",
   "metadata": {
    "id": "3312aea6-1674-4ad7-8bd9-c13e16c2e59f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hydro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import date\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df313d-e863-4f52-8089-66a9c2cdbd3a",
   "metadata": {
    "id": "80df313d-e863-4f52-8089-66a9c2cdbd3a"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65302f0f-d255-44b0-9062-fa93df07e88e",
   "metadata": {
    "id": "65302f0f-d255-44b0-9062-fa93df07e88e"
   },
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_xml(text):\n",
    "    return re.sub(r'<[^<]+?>', '', text)\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return text.replace('\\n', ' ') \n",
    "    \n",
    "\n",
    "def remove_manyspaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_xml(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_manyspaces(text)\n",
    "    return text\n",
    "\n",
    "# lemmatizatioin using Spacy to count the appearance of each word\n",
    "def space (comment):\n",
    "    doc = nlp(comment)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "def join_edited_string(edited_tweets):\n",
    "          \n",
    "    return edited_tweets.str.cat(sep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a741553c-96ab-4734-929a-17e43ebd1e2b",
   "metadata": {
    "id": "a741553c-96ab-4734-929a-17e43ebd1e2b",
    "outputId": "6340dd56-7452-435a-f650-9e77fae085f5"
   },
   "outputs": [],
   "source": [
    "def analysis_to_csv(data_path, output_path):\n",
    "\n",
    "    days=os.listdir(data_path)\n",
    "    days =[ i for i in days if i.split('.')[0].split('-')[-1]=='en']\n",
    "    data_list = []\n",
    "    for i in days:\n",
    "        file_path  = i\n",
    "        path = os.path.join(data_path, file_path)\n",
    "        df_day = pd.read_csv(path,lineterminator='\\n')\n",
    "        print(df_day)\n",
    "        data_list.append(df_day)\n",
    "    df = pd.concat(data_list)\n",
    "    df['day'] = df['created_at'].apply(lambda x: x.split(' ')[0])\n",
    "    df['hour'] = df['created_at'].apply(lambda x: x.split(' ')[1])\n",
    "    df['hour'] = df['hour'].apply(lambda x: x.split('-')[0])\n",
    "    df = df.reset_index(drop=True)\n",
    "    day_count = df.groupby('day').count()['text']\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_emoji(x))\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    df['text'] = df['text'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub('\\[[^]]*\\]', '', x))\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stop = set(stopwords.words('english'))\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop.update(punctuation)\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df['text']= df['text'].apply(clean_text)\n",
    "    df_02 = df.copy()\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    df_02['text']= df_02['text'].apply(space)\n",
    "    joined_string = join_edited_string(df['text'])\n",
    "    tokens = joined_string.split(' ')\n",
    "    joined_string1 = join_edited_string(df_02['text'])\n",
    "    tokens1 = joined_string1.split(' ')\n",
    "    unique_words = set(tokens)\n",
    "    unique_words1 = set(tokens1)\n",
    "    unique_words.update(unique_words1)\n",
    "    word_to_ind = dict((word, i) for i, word in enumerate(unique_words))\n",
    "    ind_to_word = dict((i, word) for i, word in enumerate(unique_words))\n",
    "    ncr = pd.read_excel(\"D:/data2/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx\")\n",
    "    ncr = ncr[['English (en)', 'Positive', 'Negative', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']]\n",
    "    emotions = ['Anger', 'Anticipation','Disgust','Fear', 'Joy','Sadness', 'Surprise', 'Trust']\n",
    "    df_tweets = df.copy(deep=True)\n",
    "    emo_info = {'emotion':'' , 'emo_frq': defaultdict(int) }\n",
    "    list_emotion_counts = []\n",
    "    for emotion in emotions:\n",
    "            emo_info = {}\n",
    "            emo_info['emotion'] = emotion\n",
    "            emo_info['emo_frq'] = defaultdict(int)\n",
    "            list_emotion_counts.append(emo_info)\n",
    "    nrow = df_tweets.shape[0]\n",
    "    df_emotions = pd.DataFrame(0, index=df_tweets.index, columns=emotions)\n",
    "    process_count=0\n",
    "    for i in range(nrow):\n",
    "        \n",
    "            process_count+=1\n",
    "            if process_count % 100==0:\n",
    "                print(process_count)\n",
    "                print(df_tweets.loc[i])\n",
    "            tweet = word_tokenize(df_tweets.loc[i]['text'])\n",
    "            for word in tweet:\n",
    "                word_stemmed = word\n",
    "                result = ncr[ncr['English (en)'] == word_stemmed]\n",
    "                if not result.empty:\n",
    "                    # update the tweet-emotions counts\n",
    "                    for idx, emotion in enumerate(emotions):\n",
    "                        df_emotions.at[i, emotion] += result[emotion].iloc[0]\n",
    "                        # update the frequencies dictionary list\n",
    "                        if result[emotion].iloc[0].any():\n",
    "                            try:\n",
    "                                list_emotion_counts[idx]['emo_frq'][word_to_ind[word]] += 1\n",
    "                            except:\n",
    "                                continue\n",
    "                else:\n",
    "                    doc = nlp(word)\n",
    "                    word_stemmed1 = \" \".join([token.lemma_ for token in doc]).lower()\n",
    "                    result1 = ncr[ncr['English (en)'] == word_stemmed1]\n",
    "                    if not result1.empty:\n",
    "                        # update the tweet-emotions counts\n",
    "                        for idx, emotion in enumerate(emotions):\n",
    "                            df_emotions.at[i, emotion] += result1[emotion].iloc[0]\n",
    "                            # update the frequencies dictionary list\n",
    "                            if result1[emotion].iloc[0].any():\n",
    "                                try:\n",
    "                                    list_emotion_counts[idx]['emo_frq'][word_to_ind[word_stemmed1]] += 1\n",
    "                                except:\n",
    "                                    continue\n",
    "    df_tweets = pd.concat([df_tweets, df_emotions], axis=1)\n",
    "    df_tweets.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f1434d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\covid research\\src\\local_exe\\step 3\\step_3 English analysis.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# from concurrent.futures import ProcessPoolExecutor, as_completed\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#         for future in as_completed(futures):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#             print(future.result())\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m analysis_to_csv(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcovid research\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtest_set\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms3 test input\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcovid research\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtest_set\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms3 test output\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mstep3test.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32md:\\covid research\\src\\local_exe\\step 3\\step_3 English analysis.ipynb Cell 5\u001b[0m in \u001b[0;36manalysis_to_csv\u001b[1;34m(data_path, output_path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(df_day)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data_list\u001b[39m.\u001b[39mappend(df_day)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(data_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mday\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/covid%20research/src/local_exe/step%203/step_3%20English%20analysis.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mhour\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hydro\\anaconda3\\envs\\cov\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hydro\\anaconda3\\envs\\cov\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m         objs,\n\u001b[0;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\hydro\\anaconda3\\envs\\cov\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "\n",
    "        \n",
    "#         futures = []\n",
    "#         futures.append(executor.submit(analysis_to_csv, r\"D:\\covid research\\output\", r\"D:\\covid research\\output\\step3test.csv\"))\n",
    "#         futures.append(executor.submit(analysis_to_csv, r\"D:\\covid research\\output\", r\"D:\\covid research\\output\\step3test.csv\"))\n",
    "#         for future in as_completed(futures):\n",
    "#             print(future.result())\n",
    "\n",
    "analysis_to_csv(r\"D:\\covid research\\test_set\\s3 test input\", r\"D:\\covid research\\test_set\\s3 test output\\step3test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5683bb38-c941-4406-a05f-ffccf62e8fbb"
   ],
   "name": "English analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cov')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f36b79d7a8a7815ded44fea8921e930581701a037056ac38129bf2cc427d02b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
